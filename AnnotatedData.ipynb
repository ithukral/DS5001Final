{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/ishathukral/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/ishathukral/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ishathukral/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pdfminer.high_level import extract_text\n",
    "import nltk\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk import pos_tag\n",
    "from collections import defaultdict\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import math\n",
    "import numpy as np\n",
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted Siri Guru Granth .pdf to text and saved to /Users/ishathukral/Downloads/DS5001project/sikh_text/Siri Guru Granth .txt\n"
     ]
    }
   ],
   "source": [
    "def convert_pdf_to_txt(source_folder, output_folder):\n",
    "\n",
    "    for filename in os.listdir(source_folder):\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            file_path = os.path.join(source_folder, filename)\n",
    "            text = extract_text(file_path)\n",
    "            output_file_path = os.path.join(output_folder, filename.replace('.pdf', '.txt'))\n",
    "            with open(output_file_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(text)\n",
    "            print(f\"Converted {filename} to text and saved to {output_file_path}\")\n",
    "\n",
    "source_folder = '/Users/ishathukral/Downloads/DS5001project/sikh_data'\n",
    "output_folder = '/Users/ishathukral/Downloads/DS5001project/sikh_text'\n",
    "\n",
    "convert_pdf_to_txt(source_folder, output_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_file_path</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>length</th>\n",
       "      <th>num_tokens</th>\n",
       "      <th>num_sections</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>book_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/Users/ishathukral/Downloads/DS5001project/sik...</td>\n",
       "      <td>Multiple Gurus and Saints</td>\n",
       "      <td>Guru Granth Sahib</td>\n",
       "      <td>1607</td>\n",
       "      <td>5051999</td>\n",
       "      <td>1016165</td>\n",
       "      <td>370</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          source_file_path  \\\n",
       "book_id                                                      \n",
       "1        /Users/ishathukral/Downloads/DS5001project/sik...   \n",
       "\n",
       "                            author              title  date   length  \\\n",
       "book_id                                                                \n",
       "1        Multiple Gurus and Saints  Guru Granth Sahib  1607  5051999   \n",
       "\n",
       "         num_tokens  num_sections  \n",
       "book_id                            \n",
       "1           1016165           370  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def preprocess_text(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    return text\n",
    "\n",
    "def tokenize_and_count(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    return len(tokens)\n",
    "\n",
    "def find_sections(text):\n",
    "    pattern = re.compile(r'\\bRAAG\\b.*?(?=\\bRAAG\\b|\\Z)', re.DOTALL | re.IGNORECASE)\n",
    "    return len(re.findall(pattern, text))\n",
    "\n",
    "source_file_path = '/Users/ishathukral/Downloads/DS5001project/sikh_text/SiriGuruGranth.txt'\n",
    "\n",
    "text = preprocess_text(source_file_path)\n",
    "num_tokens = tokenize_and_count(text)\n",
    "num_sections = find_sections(text)  \n",
    "\n",
    "LIB = pd.DataFrame({\n",
    "    'source_file_path': [source_file_path],\n",
    "    'author': ['Multiple Gurus and Saints'],\n",
    "    'title': ['Guru Granth Sahib'],\n",
    "    'date': ['1607'],\n",
    "    'length': [len(text)],\n",
    "    'num_tokens': [num_tokens],\n",
    "    'num_sections': [num_sections]  \n",
    "})\n",
    "\n",
    "LIB.index = [1]  #  this is the only document\n",
    "LIB.index.name = 'book_id'\n",
    "\n",
    "\n",
    "LIB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIB.to_csv('/Users/ishathukral/Downloads/DS5001project/output/LIB.csv', sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>para_id</th>\n",
       "      <th>sent_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>token_str</th>\n",
       "      <th>term_str</th>\n",
       "      <th>pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ÓÓÓÓÓÓÓÓÓÓÓ</td>\n",
       "      <td>óóóóóóóóóóó</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>CD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>ÓÓÓÓÓÓÓÓÓ</td>\n",
       "      <td>óóóóóóóóó</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ONE</td>\n",
       "      <td>one</td>\n",
       "      <td>CD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>UNIVERSAL</td>\n",
       "      <td>universal</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1015766</th>\n",
       "      <td>208</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>||</td>\n",
       "      <td>||</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1015767</th>\n",
       "      <td>208</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>CD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1015768</th>\n",
       "      <td>208</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>||</td>\n",
       "      <td>||</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1015769</th>\n",
       "      <td>208</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>CD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1015770</th>\n",
       "      <td>208</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>||</td>\n",
       "      <td>||</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1015771 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         doc_id  para_id  sent_id  token_id    token_str     term_str pos\n",
       "0             1        0        0         0  ÓÓÓÓÓÓÓÓÓÓÓ  óóóóóóóóóóó  NN\n",
       "1             1        0        0         1            1            1  CD\n",
       "2             1        0        0         2    ÓÓÓÓÓÓÓÓÓ    óóóóóóóóó  NN\n",
       "3             1        2        0         0          ONE          one  CD\n",
       "4             1        2        0         1    UNIVERSAL    universal  NN\n",
       "...         ...      ...      ...       ...          ...          ...  ..\n",
       "1015766     208       60        1         0           ||           ||  NN\n",
       "1015767     208       60        1         1            1            1  CD\n",
       "1015768     208       60        1         2           ||           ||  NN\n",
       "1015769     208       60        1         3            1            1  CD\n",
       "1015770     208       60        1         4           ||           ||  NN\n",
       "\n",
       "[1015771 rows x 7 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_text(source_file_path):\n",
    "    with open(source_file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "\n",
    "    sections = re.split(r'RAAG \\w+', text)\n",
    "    corpus_data = []\n",
    "    doc_id = 0\n",
    "    for section in sections:\n",
    "        if section.strip(): \n",
    "            doc_id += 1  \n",
    "            for para_id, paragraph in enumerate(section.split('\\n')):\n",
    "                for sent_id, sentence in enumerate(sent_tokenize(paragraph)):\n",
    "                    for token_id, token in enumerate(word_tokenize(sentence)):\n",
    "                        pos = pos_tag([token])[0][1]  # Get POS tag for each token\n",
    "                        corpus_data.append((doc_id, para_id, sent_id, token_id, token, token.lower(), pos))\n",
    "                    \n",
    "    columns = ['doc_id', 'para_id', 'sent_id', 'token_id', 'token_str', 'term_str', 'pos']\n",
    "    return pd.DataFrame(corpus_data, columns=columns)\n",
    "\n",
    "source_file_path = '/Users/ishathukral/Downloads/DS5001project/sikh_text/SiriGuruGranth.txt'\n",
    "CORPUS = preprocess_text(source_file_path)\n",
    "\n",
    "\n",
    "CORPUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "CORPUS.to_csv('/Users/ishathukral/Downloads/DS5001project/output/CORPUS.csv', sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term_str</th>\n",
       "      <th>n</th>\n",
       "      <th>df</th>\n",
       "      <th>idf</th>\n",
       "      <th>dfidf</th>\n",
       "      <th>porter_stem</th>\n",
       "      <th>max_pos</th>\n",
       "      <th>stop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1465</th>\n",
       "      <td>respect</td>\n",
       "      <td>36</td>\n",
       "      <td>208</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>respect</td>\n",
       "      <td>NN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>one</td>\n",
       "      <td>6479</td>\n",
       "      <td>208</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>one</td>\n",
       "      <td>CD</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>naam</td>\n",
       "      <td>2696</td>\n",
       "      <td>208</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>naam</td>\n",
       "      <td>NN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>shall</td>\n",
       "      <td>2709</td>\n",
       "      <td>208</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>shall</td>\n",
       "      <td>MD</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>all-pervading</td>\n",
       "      <td>137</td>\n",
       "      <td>207</td>\n",
       "      <td>1.004796</td>\n",
       "      <td>207.992808</td>\n",
       "      <td>all-pervad</td>\n",
       "      <td>NN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7708</th>\n",
       "      <td>unconcerned</td>\n",
       "      <td>1</td>\n",
       "      <td>205</td>\n",
       "      <td>1.014458</td>\n",
       "      <td>207.963907</td>\n",
       "      <td>unconcern</td>\n",
       "      <td>JJ</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6856</th>\n",
       "      <td>leafy</td>\n",
       "      <td>2</td>\n",
       "      <td>204</td>\n",
       "      <td>1.019324</td>\n",
       "      <td>207.942152</td>\n",
       "      <td>leafi</td>\n",
       "      <td>NN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1466</th>\n",
       "      <td>weight</td>\n",
       "      <td>36</td>\n",
       "      <td>202</td>\n",
       "      <td>1.029128</td>\n",
       "      <td>207.883911</td>\n",
       "      <td>weight</td>\n",
       "      <td>NN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10954</th>\n",
       "      <td>ever-young</td>\n",
       "      <td>1</td>\n",
       "      <td>199</td>\n",
       "      <td>1.044017</td>\n",
       "      <td>207.759360</td>\n",
       "      <td>ever-young</td>\n",
       "      <td>NN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5363</th>\n",
       "      <td>atonement</td>\n",
       "      <td>3</td>\n",
       "      <td>198</td>\n",
       "      <td>1.049029</td>\n",
       "      <td>207.707827</td>\n",
       "      <td>aton</td>\n",
       "      <td>NN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5550</th>\n",
       "      <td>alms</td>\n",
       "      <td>3</td>\n",
       "      <td>198</td>\n",
       "      <td>1.049029</td>\n",
       "      <td>207.707827</td>\n",
       "      <td>alm</td>\n",
       "      <td>NNS</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5431</th>\n",
       "      <td>severe</td>\n",
       "      <td>3</td>\n",
       "      <td>193</td>\n",
       "      <td>1.074476</td>\n",
       "      <td>207.373886</td>\n",
       "      <td>sever</td>\n",
       "      <td>JJ</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>679</th>\n",
       "      <td>nanak‟s</td>\n",
       "      <td>110</td>\n",
       "      <td>193</td>\n",
       "      <td>1.074476</td>\n",
       "      <td>207.373886</td>\n",
       "      <td>nanak‟</td>\n",
       "      <td>NN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3409</th>\n",
       "      <td>dynasty</td>\n",
       "      <td>8</td>\n",
       "      <td>192</td>\n",
       "      <td>1.079644</td>\n",
       "      <td>207.291660</td>\n",
       "      <td>dynasti</td>\n",
       "      <td>NN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>793</th>\n",
       "      <td>hundreds</td>\n",
       "      <td>89</td>\n",
       "      <td>190</td>\n",
       "      <td>1.090061</td>\n",
       "      <td>207.111557</td>\n",
       "      <td>hundr</td>\n",
       "      <td>NNS</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11149</th>\n",
       "      <td>winding</td>\n",
       "      <td>1</td>\n",
       "      <td>186</td>\n",
       "      <td>1.111226</td>\n",
       "      <td>206.687968</td>\n",
       "      <td>wind</td>\n",
       "      <td>VBG</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6878</th>\n",
       "      <td>attempts</td>\n",
       "      <td>2</td>\n",
       "      <td>184</td>\n",
       "      <td>1.121978</td>\n",
       "      <td>206.444031</td>\n",
       "      <td>attempt</td>\n",
       "      <td>NNS</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11852</th>\n",
       "      <td>swimmer</td>\n",
       "      <td>1</td>\n",
       "      <td>182</td>\n",
       "      <td>1.132848</td>\n",
       "      <td>206.178354</td>\n",
       "      <td>swimmer</td>\n",
       "      <td>NN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7246</th>\n",
       "      <td>balraja‟s</td>\n",
       "      <td>1</td>\n",
       "      <td>182</td>\n",
       "      <td>1.132848</td>\n",
       "      <td>206.178354</td>\n",
       "      <td>balraja‟</td>\n",
       "      <td>NN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7491</th>\n",
       "      <td>exposing</td>\n",
       "      <td>1</td>\n",
       "      <td>181</td>\n",
       "      <td>1.138328</td>\n",
       "      <td>206.037289</td>\n",
       "      <td>expos</td>\n",
       "      <td>VBG</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            term_str     n   df       idf       dfidf porter_stem max_pos  \\\n",
       "1465         respect    36  208  1.000000  208.000000     respect      NN   \n",
       "5                one  6479  208  1.000000  208.000000         one      CD   \n",
       "18              naam  2696  208  1.000000  208.000000        naam      NN   \n",
       "17             shall  2709  208  1.000000  208.000000       shall      MD   \n",
       "567    all-pervading   137  207  1.004796  207.992808  all-pervad      NN   \n",
       "7708     unconcerned     1  205  1.014458  207.963907   unconcern      JJ   \n",
       "6856           leafy     2  204  1.019324  207.942152       leafi      NN   \n",
       "1466          weight    36  202  1.029128  207.883911      weight      NN   \n",
       "10954     ever-young     1  199  1.044017  207.759360  ever-young      NN   \n",
       "5363       atonement     3  198  1.049029  207.707827        aton      NN   \n",
       "5550            alms     3  198  1.049029  207.707827         alm     NNS   \n",
       "5431          severe     3  193  1.074476  207.373886       sever      JJ   \n",
       "679          nanak‟s   110  193  1.074476  207.373886      nanak‟      NN   \n",
       "3409         dynasty     8  192  1.079644  207.291660     dynasti      NN   \n",
       "793         hundreds    89  190  1.090061  207.111557       hundr     NNS   \n",
       "11149        winding     1  186  1.111226  206.687968        wind     VBG   \n",
       "6878        attempts     2  184  1.121978  206.444031     attempt     NNS   \n",
       "11852        swimmer     1  182  1.132848  206.178354     swimmer      NN   \n",
       "7246       balraja‟s     1  182  1.132848  206.178354    balraja‟      NN   \n",
       "7491        exposing     1  181  1.138328  206.037289       expos     VBG   \n",
       "\n",
       "        stop  \n",
       "1465   False  \n",
       "5      False  \n",
       "18     False  \n",
       "17     False  \n",
       "567    False  \n",
       "7708   False  \n",
       "6856   False  \n",
       "1466   False  \n",
       "10954  False  \n",
       "5363   False  \n",
       "5550   False  \n",
       "5431   False  \n",
       "679    False  \n",
       "3409   False  \n",
       "793    False  \n",
       "11149  False  \n",
       "6878   False  \n",
       "11852  False  \n",
       "7246   False  \n",
       "7491   False  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "CORPUS = CORPUS[~CORPUS['term_str'].isin(['óóóóóóóóóóó', '||'])]  # Removing specific unwanted tokens\n",
    "\n",
    "# Remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "CORPUS = CORPUS[~CORPUS['term_str'].isin(stop_words)]\n",
    "\n",
    "VOCAB = CORPUS['term_str'].value_counts().reset_index()\n",
    "VOCAB.columns = ['term_str', 'n']\n",
    "\n",
    "VOCAB['df'] = CORPUS.groupby('term_str')['doc_id'].nunique().reset_index(drop=True)\n",
    "\n",
    "N_docs = CORPUS['doc_id'].nunique()\n",
    "\n",
    "VOCAB['idf'] = np.log((N_docs + 1) / (VOCAB['df'] + 1)) + 1\n",
    "\n",
    "VOCAB['dfidf'] = VOCAB['df'] * VOCAB['idf']\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "VOCAB['porter_stem'] = VOCAB['term_str'].apply(lambda x: stemmer.stem(x))\n",
    "\n",
    "# Calculate max POS for each term\n",
    "def get_max_pos(term):\n",
    "    pos_list = CORPUS[CORPUS['term_str'] == term]['pos']\n",
    "    return pos_list.mode()[0] if not pos_list.empty else 'NN'  # Default to 'NN' if no mode found\n",
    "\n",
    "VOCAB['max_pos'] = VOCAB['term_str'].apply(get_max_pos)\n",
    "VOCAB['stop'] = VOCAB['term_str'].apply(lambda x: x in stop_words)\n",
    "\n",
    "VOCAB.sort_values(by='dfidf', ascending=False).head(20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB.to_csv('/Users/ishathukral/Downloads/DS5001project/output/VOCAB.csv', sep='|', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
